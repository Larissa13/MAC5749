{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sarpy.datasets import load_emnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reshape training: 100%|██████████| 60000/60000 [00:01<00:00, 47107.66it/s]\n",
      "reshape testing: 100%|██████████| 10000/10000 [00:00<00:00, 35970.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 50000\n",
      "Test size: 10000\n",
      "Validation size: 10000\n",
      "# classes: 10\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, X_valid, y_valid, mapping, nb_classes = load_emnist('mnist', validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ricardo/miniconda3/envs/sarpy/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DigitsDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].reshape((1, 28, 28))\n",
    "        label = np.asscalar(self.y[idx])\n",
    "        sample = {'image': image, 'label': label}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        return {'image': torch.from_numpy(image),\n",
    "#         return {'image': torch.from_numpy(np.flip(image.transpose((0, 1, 2)),axis=0).copy()),\n",
    "                'label': torch.tensor(label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size, inverse_resize_rate, transforms=[]):\n",
    "    transform_train = Compose([])\n",
    "    for t in transforms:\n",
    "        transform_train.transforms.append(t)\n",
    "    transform_train.transforms.append(ToTensor())\n",
    "    \n",
    "    transform_test = Compose([\n",
    "        ToTensor()\n",
    "    ])\n",
    "\n",
    "    digits_dataset_train = DigitsDataset(X_train, y_train, transform=transform_train)\n",
    "    digits_dataset_test = DigitsDataset(X_test, y_test, transform=transform_test)\n",
    "\n",
    "    dataloader_train = DataLoader(digits_dataset_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    dataloader_test = DataLoader(digits_dataset_test, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    return dataloader_train, dataloader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)        \n",
    "        self.dropout1 = nn.Dropout2d(p=0.25)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64*(n//4)**2, 200)\n",
    "        self.dropout2 = nn.Dropout(p=0.25)\n",
    "        \n",
    "        self.fc2 = nn.Linear(200, 100)\n",
    "        self.dropout3 = nn.Dropout(p=0.25)\n",
    "        \n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        x = F.relu(self.conv1(x)) \n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))        \n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = x.view(-1, 64 * (self.n // 4) ** 2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def run_training(dataloader_train, dataloader_test, image_size, batch_size, epochs):\n",
    "    net = Net(n=image_size)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dataloader_train, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data['image'], data['label']\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if batch_size*(i+1) % 5000 == 0:    # print every 5000 mini-batches\n",
    "                correct = 0\n",
    "                total = 1\n",
    "                with torch.no_grad():\n",
    "                    for data in dataloader_test:\n",
    "                        inputs, labels = data['image'], data['label']\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        outputs = net(inputs)\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "\n",
    "                print('[%d, %5d] loss: %.3f test acc: %.3f' %\n",
    "                      (epoch + 1, batch_size*(i+1), running_loss / 5000, (100*correct/total)))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  5000] loss: 0.150 test acc: 91.281\n",
      "[1, 10000] loss: 0.072 test acc: 90.051\n",
      "[1, 15000] loss: 0.054 test acc: 92.991\n",
      "[1, 20000] loss: 0.049 test acc: 94.691\n",
      "[1, 25000] loss: 0.051 test acc: 93.051\n",
      "[1, 30000] loss: 0.042 test acc: 94.411\n",
      "[1, 35000] loss: 0.043 test acc: 94.301\n",
      "[1, 40000] loss: 0.044 test acc: 95.510\n",
      "[1, 45000] loss: 0.040 test acc: 95.090\n",
      "[1, 50000] loss: 0.032 test acc: 95.620\n",
      "[2,  5000] loss: 0.035 test acc: 95.750\n",
      "[2, 10000] loss: 0.037 test acc: 94.331\n",
      "[2, 15000] loss: 0.033 test acc: 95.640\n",
      "[2, 20000] loss: 0.032 test acc: 95.830\n",
      "[2, 25000] loss: 0.036 test acc: 94.701\n",
      "[2, 30000] loss: 0.034 test acc: 96.380\n",
      "[2, 35000] loss: 0.030 test acc: 95.820\n",
      "[2, 40000] loss: 0.034 test acc: 96.410\n",
      "[2, 45000] loss: 0.037 test acc: 95.650\n",
      "[2, 50000] loss: 0.040 test acc: 95.930\n",
      "Finished Training\n",
      "CPU times: user 2min 38s, sys: 13.6 s, total: 2min 52s\n",
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataloader_train, dataloader_test = get_dataloaders(batch_size=5, inverse_resize_rate=1)\n",
    "run_training(dataloader_train=dataloader_train,\n",
    "             dataloader_test=dataloader_test,\n",
    "             image_size=28,\n",
    "             batch_size=5,\n",
    "             epochs=2\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ImageAugmentation(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.seq = iaa.Sequential([\n",
    "#             iaa.Fliplr(0.5), # horizontal flips\n",
    "            iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "            # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "            # But we only blur about 50% of all images.\n",
    "            iaa.Sometimes(0.5,\n",
    "                iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "            ),\n",
    "            # Strengthen or weaken the contrast in each image.\n",
    "            iaa.ContrastNormalization((0.75, 1.5)),\n",
    "            # Add gaussian noise.\n",
    "            # For 50% of all images, we sample the noise once per pixel.\n",
    "            # For the other 50% of all images, we sample the noise per pixel AND\n",
    "            # channel. This can change the color (not only brightness) of the\n",
    "            # pixels.\n",
    "            iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "            # Make some images brighter and some darker.\n",
    "            # In 20% of all cases, we sample the multiplier once per channel,\n",
    "            # which can end up changing the color of the images.\n",
    "            iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "            # Apply affine transformations to each image.\n",
    "            # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "            iaa.Affine(\n",
    "                scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "                translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "                rotate=(-25, 25),\n",
    "                shear=(-8, 8)\n",
    "            )\n",
    "        ], random_order=True) # apply augmenters in random order\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        \n",
    "        return {'image': self.seq.augment_images(image),\n",
    "                'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  5000] loss: 0.418 test acc: 57.124\n",
      "[1, 10000] loss: 0.310 test acc: 70.803\n",
      "[1, 15000] loss: 0.233 test acc: 76.952\n",
      "[1, 20000] loss: 0.211 test acc: 84.692\n",
      "[1, 25000] loss: 0.187 test acc: 83.882\n",
      "[1, 30000] loss: 0.174 test acc: 87.331\n",
      "[1, 35000] loss: 0.166 test acc: 86.581\n",
      "[1, 40000] loss: 0.153 test acc: 86.671\n",
      "[1, 45000] loss: 0.157 test acc: 89.031\n",
      "[1, 50000] loss: 0.143 test acc: 88.281\n",
      "[2,  5000] loss: 0.143 test acc: 86.191\n",
      "[2, 10000] loss: 0.132 test acc: 89.521\n",
      "[2, 15000] loss: 0.122 test acc: 89.411\n",
      "[2, 20000] loss: 0.134 test acc: 89.081\n",
      "[2, 25000] loss: 0.126 test acc: 90.561\n",
      "[2, 30000] loss: 0.125 test acc: 89.701\n",
      "[2, 35000] loss: 0.130 test acc: 91.311\n",
      "[2, 40000] loss: 0.125 test acc: 89.271\n",
      "[2, 45000] loss: 0.112 test acc: 91.311\n",
      "[2, 50000] loss: 0.120 test acc: 90.051\n",
      "[3,  5000] loss: 0.109 test acc: 91.851\n",
      "[3, 10000] loss: 0.109 test acc: 92.241\n",
      "[3, 15000] loss: 0.106 test acc: 88.851\n",
      "[3, 20000] loss: 0.110 test acc: 91.031\n",
      "[3, 25000] loss: 0.102 test acc: 92.371\n",
      "[3, 30000] loss: 0.111 test acc: 91.281\n",
      "[3, 35000] loss: 0.102 test acc: 92.201\n",
      "[3, 40000] loss: 0.103 test acc: 93.791\n",
      "[3, 45000] loss: 0.102 test acc: 92.371\n",
      "[3, 50000] loss: 0.100 test acc: 93.261\n",
      "[4,  5000] loss: 0.098 test acc: 92.531\n",
      "[4, 10000] loss: 0.095 test acc: 92.051\n",
      "[4, 15000] loss: 0.100 test acc: 91.551\n",
      "[4, 20000] loss: 0.092 test acc: 91.721\n",
      "[4, 25000] loss: 0.097 test acc: 91.841\n",
      "[4, 30000] loss: 0.104 test acc: 92.641\n",
      "[4, 35000] loss: 0.091 test acc: 93.671\n",
      "[4, 40000] loss: 0.095 test acc: 92.481\n",
      "[4, 45000] loss: 0.102 test acc: 93.741\n",
      "[4, 50000] loss: 0.095 test acc: 93.951\n",
      "[5,  5000] loss: 0.093 test acc: 92.341\n",
      "[5, 10000] loss: 0.101 test acc: 93.611\n",
      "[5, 15000] loss: 0.092 test acc: 93.851\n",
      "[5, 20000] loss: 0.091 test acc: 94.131\n",
      "[5, 25000] loss: 0.093 test acc: 94.091\n",
      "[5, 30000] loss: 0.100 test acc: 93.201\n",
      "[5, 35000] loss: 0.090 test acc: 94.311\n",
      "[5, 40000] loss: 0.097 test acc: 94.271\n",
      "[5, 45000] loss: 0.095 test acc: 92.361\n",
      "[5, 50000] loss: 0.091 test acc: 93.891\n",
      "[6,  5000] loss: 0.086 test acc: 94.291\n",
      "[6, 10000] loss: 0.089 test acc: 93.471\n",
      "[6, 15000] loss: 0.085 test acc: 93.811\n",
      "[6, 20000] loss: 0.086 test acc: 93.921\n",
      "[6, 25000] loss: 0.089 test acc: 92.361\n",
      "[6, 30000] loss: 0.089 test acc: 94.341\n",
      "[6, 35000] loss: 0.090 test acc: 92.281\n",
      "[6, 40000] loss: 0.086 test acc: 93.591\n",
      "[6, 45000] loss: 0.091 test acc: 94.541\n",
      "[6, 50000] loss: 0.086 test acc: 92.951\n",
      "[7,  5000] loss: 0.094 test acc: 94.421\n",
      "[7, 10000] loss: 0.092 test acc: 93.031\n",
      "[7, 15000] loss: 0.088 test acc: 92.561\n",
      "[7, 20000] loss: 0.093 test acc: 93.071\n",
      "[7, 25000] loss: 0.089 test acc: 94.301\n",
      "[7, 30000] loss: 0.087 test acc: 94.601\n",
      "[7, 35000] loss: 0.081 test acc: 94.451\n",
      "[7, 40000] loss: 0.091 test acc: 94.241\n",
      "[7, 45000] loss: 0.090 test acc: 94.591\n",
      "[7, 50000] loss: 0.085 test acc: 94.161\n",
      "[8,  5000] loss: 0.087 test acc: 94.021\n",
      "[8, 10000] loss: 0.088 test acc: 94.351\n",
      "[8, 15000] loss: 0.094 test acc: 93.701\n",
      "[8, 20000] loss: 0.086 test acc: 93.871\n",
      "[8, 25000] loss: 0.084 test acc: 93.091\n",
      "[8, 30000] loss: 0.086 test acc: 94.331\n",
      "[8, 35000] loss: 0.093 test acc: 93.011\n",
      "[8, 40000] loss: 0.087 test acc: 93.291\n",
      "[8, 45000] loss: 0.083 test acc: 93.281\n",
      "[8, 50000] loss: 0.088 test acc: 93.881\n",
      "[9,  5000] loss: 0.084 test acc: 94.361\n",
      "[9, 10000] loss: 0.086 test acc: 94.121\n",
      "[9, 15000] loss: 0.085 test acc: 93.671\n",
      "[9, 20000] loss: 0.078 test acc: 93.241\n",
      "[9, 25000] loss: 0.091 test acc: 93.131\n",
      "[9, 30000] loss: 0.088 test acc: 94.381\n",
      "[9, 35000] loss: 0.087 test acc: 93.361\n",
      "[9, 40000] loss: 0.087 test acc: 94.191\n",
      "[9, 45000] loss: 0.081 test acc: 94.341\n",
      "[9, 50000] loss: 0.092 test acc: 94.181\n",
      "[10,  5000] loss: 0.077 test acc: 95.150\n",
      "[10, 10000] loss: 0.076 test acc: 94.551\n",
      "[10, 15000] loss: 0.086 test acc: 93.751\n",
      "[10, 20000] loss: 0.081 test acc: 94.401\n",
      "[10, 25000] loss: 0.086 test acc: 93.671\n",
      "[10, 30000] loss: 0.090 test acc: 94.771\n",
      "[10, 35000] loss: 0.089 test acc: 94.621\n",
      "[10, 40000] loss: 0.077 test acc: 94.961\n",
      "[10, 45000] loss: 0.084 test acc: 94.161\n",
      "[10, 50000] loss: 0.086 test acc: 94.711\n",
      "Finished Training\n",
      "CPU times: user 12min 2s, sys: 1min, total: 13min 3s\n",
      "Wall time: 13min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataloader_train, dataloader_test = get_dataloaders(batch_size=5, inverse_resize_rate=1, transforms=[ImageAugmentation()])\n",
    "run_training(dataloader_train=dataloader_train,\n",
    "             dataloader_test=dataloader_test,\n",
    "             image_size=28,\n",
    "             batch_size=5,\n",
    "             epochs=10\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: tensor(5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloader_train, dataloader_test = get_dataloaders(batch_size=5, inverse_resize_rate=1, transforms=[ImageAugmentation()])\n",
    "\n",
    "for x in dataloader_train:\n",
    "    print('label:', x['label'][0])\n",
    "    plt.imshow(x['image'][0].numpy().reshape((28, 28)), cmap='gray')    \n",
    "    plt.show()\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
